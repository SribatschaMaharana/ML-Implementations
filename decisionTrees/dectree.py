# -*- coding: utf-8 -*-
"""DecTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ocMj7rbhGaM7R8_Q5MjKb_ir7EGIGSDS
"""

import numpy as np
import pandas as pd
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
epsilon = 1e-6

class Node:
    def __init__(self):
        self.attribute = None  # attribute string for internal node
        self.child0 = None
        self.child1 = None
        self.child2 = None
        self.label = None  # label to assign for leaf node

#DECISION TREE FUNCTION--------------------------------------------------------------------------------------------------

def decision_tree(D, L):
  node = Node()

  # • stopping critera • If all instances in D belong to the same class, y Define node N as a leaf node labeled with y and return it
  if len(D.iloc[:, -1].unique()) == 1:
    node.label = D.iloc[:, -1].unique()[0]
    return node

  # • If there are no more attributes that can be tested (i.e., if L = empty) Define node N as a leaf node labeled with the majority class in D, and return it
  if len(L) == 0:
        node.label = D.iloc[:, -1].mode()[0]  # assign the most frequent label as the node label
        return node
  # • stopping criteria done

  # • Let A be the best attribute to split the dataset D // Select splitting attribute according to some criterion
  A = bestAttribute(D, L) #best attribute = info gain is highest

  # • Define node N as a decision node that tests attribute A
  node.attribute = A

  # • Remove A from the list of attributes that can still be tested: L := L - {A}
  L.remove(A)

  # • Create Subtrees
  # • Let V be a list with all different values of attribute A considering the instances in dataset D • For each attribute value v # V:
  # • Let Dv be the partition of D containing all instances whose attribute A = v

  dv0 = D[D[A]==0]
  dv1 = D[D[A]==1]
  dv2 = D[D[A]==2]

  # • If Dv is empty
  # • Let Tv be a leaf node labeled with the majority class in D
  # • Else
  # • Let Tv be a sub-tree responsible for classifying the instances in Dv: Tv := decision_tree( Dv, L )
  # • Create an edge from node N to the root of Tv , where the edge is labeled with attribute value v

  if len(dv0) == 0:
    node.child0 = Node()
    node.child0.label = D.iloc[:,-1].mode()[0]
  else:
    node.child0 = decision_tree(dv0, L)

  if len(dv1) == 0:
    node.child1 = Node()
    node.child1.label = D.iloc[:,-1].mode()[0]
  else:
    node.child1 = decision_tree(dv1, L)

  if len(dv2) == 0:
    node.child2 = Node()
    node.child2.label = D.iloc[:,-1].mode()[0]
  else:
    node.child2 = decision_tree(dv2, L)

  # • Return N
  return node

#BEST ATTRIBUTE FUNCTION-------------------------------------------------------------------------------------------------
def bestAttribute(D, L): #implement information gain

  infoGain = float('-inf')
  bestAttr = None
  ogEntropy = entropy(D)
  for A in L:
    entropy0 = entropy(D[D[A]==0])
    entropy1 = entropy(D[D[A]==1])
    entropy2 = entropy(D[D[A]==2])
    avgEntropy = ((len(D[D[A]==0])/len(D))*entropy0)+((len(D[D[A]==1])/len(D))*entropy1)+((len(D[D[A]==2])/len(D))*entropy2)

    curGain = ogEntropy - avgEntropy
    if curGain > infoGain:
      infoGain = curGain
      bestAttr = A

  return bestAttr

#ENTROPY FUNCTION--------------------------------------------------------------------------------------------------------
def entropy(D):

  if(len(D) == 0):
    return 0

  #count how many there are
  demo = len(D[D['target']==0])
  repub = len(D[D['target']==1])

  probRepub = repub/ (len(D))
  probDemo = demo/ (len(D))

  if (probDemo == 0):
    return -(probRepub * np.log2(probRepub))

  if (probRepub== 0):
    return -(probDemo * np.log2(probDemo))

  #entropy formula
  ent = (-probDemo * np.log2(probDemo)) - (probRepub * np.log2(probRepub))
  return ent

#ACCURACY FUNCTION-------------------------------------------------------------------------------------------------------
def isTrue(insta, node):
  if node.label is None:
    if(insta[node.attribute]==0):
      return isTrue(insta,node.child0)
    if(insta[node.attribute]==1):
      return isTrue(insta,node.child1)
    if(insta[node.attribute]==2):
      return isTrue(insta,node.child2)
  else:
    if (node.label == insta['target']):
      return True
    else:
      return False

def accuracy(decisionTree, dataset):
  acc = 0
  for idx in range(len(dataset)):
    if (isTrue(dataset.iloc[idx], decisionTree)==True):
      acc = acc+1

  result = acc/len(dataset)
  return result

#TRAINER FUNCTION--------------------------------------------------------------------------------------------------------------
trainedAccuracy = []
testedAccuracy = []
# • read csv file
df = pd.read_csv('house_votes_84.csv')

#df.columns = array with column names
for _ in range(100):

    data = shuffle(df)
    L = list(df.columns.values)
    L.pop()
    # • split dataset
    trainSet, testSet = train_test_split(data, test_size=0.2, random_state=3, shuffle=True)

    decTree = decision_tree(trainSet, L)

    curTrainAccuracy = accuracy(decTree, trainSet)
    trainedAccuracy.append(curTrainAccuracy)

    curTestAccuracy = accuracy(decTree, testSet)
    testedAccuracy.append(curTestAccuracy)

print("trained accuracy average: ",np.mean(trainedAccuracy))
print("tested accuracy average: ",np.mean(testedAccuracy))

print("trained accuracy standard deviation: ",np.std(trainedAccuracy))
print("tested accuracy standard deviation: ",np.std(testedAccuracy))

#PLOTTER FUNCTION--------------------------------------------------------------------------------------------------------------
plt.figure(figsize=(10, 5))
plt.hist(trainedAccuracy, bins=20, alpha=0.5, color='blue')
plt.xlabel('Accuracy')
plt.ylabel('Frequency')
plt.title('Training Accuracy Distribution')
plt.grid(True)
plt.show()


plt.figure(figsize=(10, 5))
plt.hist(testedAccuracy, bins=20, alpha=0.5, color='green')
plt.xlabel('Accuracy')
plt.ylabel('Frequency')
plt.title('Testing Accuracy Distribution')
plt.grid(True)
plt.show()