# -*- coding: utf-8 -*-
"""DecTreeGini.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xLj3F4u9l2Yvb2lcqg7KEC8sIqEIr26r
"""

import numpy as np
import pandas as pd
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
epsilon = 1e-6
class Node:
    def __init__(self):
        self.attribute = None  # attribute string for internal node
        self.child0 = None
        self.child1 = None
        self.child2 = None
        self.label = None  # Label to assign for leaf node


#DECISION TREE FUNCTION--------------------------------------------------------------------------------------------------

def decision_tree(D, L):
  node = Node()

  # • stopping critera • If all instances in D belong to the same class, y Define node N as a leaf node labeled with y and return it
  if len(D.iloc[:, -1].unique()) == 1:
    node.label = D.iloc[:, -1].unique()[0]
    return node

  # • If there are no more attributes that can be tested (i.e., if L = empty) Define node N as a leaf node labeled with the majority class in D, and return it
  if len(L) == 0:
        node.label = D.iloc[:, -1].mode()[0]  # assign the most frequent label as the node label
        return node
  # • stopping criteria done


  # • Let A be the best attribute to split the dataset D // Select splitting attribute according to some criterion
  A = gini(D, L) #best attribute = gini index lowest

  # • Define node N as a decision node that tests attribute A
  node.attribute = A

  # • Remove A from the list of attributes that can still be tested: L := L - {A}
  L.remove(A)

  # • Create Subtrees
  # • Let V be a list with all different values of attribute A considering the instances in dataset D • For each attribute value v # V:
  # • Let Dv be the partition of D containing all instances whose attribute A = v

  dv0 = D[D[A]==0]
  dv1 = D[D[A]==1]
  dv2 = D[D[A]==2]

  # • If Dv is empty
  # • Let Tv be a leaf node labeled with the majority class in D
  # • Else
  # • Let Tv be a sub-tree responsible for classifying the instances in Dv: Tv := decision_tree( Dv, L )
  # • Create an edge from node N to the root of Tv , where the edge is labeled with attribute value v

  if len(dv0) == 0:
    node.child0 = Node()
    node.child0.label = D.iloc[:,-1].mode()[0]
  else:
    node.child0 = decision_tree(dv0, L)

  if len(dv1) == 0:
    node.child1 = Node()
    node.child1.label = D.iloc[:,-1].mode()[0]
  else:
    node.child1 = decision_tree(dv1, L)

  if len(dv2) == 0:
    node.child2 = Node()
    node.child2.label = D.iloc[:,-1].mode()[0]
  else:
    node.child2 = decision_tree(dv2, L)

  # • Return N
  return node


#ACCURACY FUNCTION-------------------------------------------------------------------------------------------------------
def isTrue(insta, node):
  if node.label is None:
    if(insta[node.attribute]==0):
      return isTrue(insta,node.child0)
    if(insta[node.attribute]==1):
      return isTrue(insta,node.child1)
    if(insta[node.attribute]==2):
      return isTrue(insta,node.child2)
  else:
    if (node.label == insta['target']):
      return True
    else:
      return False

def accuracy(decisionTree, dataset):
  acc = 0
  for idx in range(len(dataset)):
    if (isTrue(dataset.iloc[idx], decisionTree)==True):
      acc = acc+1

  result = acc/len(dataset)
  return result
#GINI--------------------------------------------------------------------------------------------------------------------

def gini(D, L):
    giniIndex = float('inf')
    bestAttr = None

    for A in L:
        giniSum = 0
        values = D[A].unique()

        for v in values:
            Dv = D[D[A] == v]
            giniSum += (len(Dv) / len(D)) * giniVal(Dv)
              
        if giniSum < giniIndex:
            giniIndex = giniSum
            bestAttr = A

    return bestAttr

def giniVal(D):

    if len(D) == 0:
        return 0

    V = len(D['target'].unique())
    probi = [len(D[D['target'] == c]) / len(D) for c in range(V)]
    giniVal = 1 - sum(p ** 2 for p in probi)

    return giniVal

#TRAINER FUNCTION--------------------------------------------------------------------------------------------------------------

trainedAccuracy = []
testedAccuracy = []
#df.columns = array with column names


# • read csv file
df = pd.read_csv('house_votes_84.csv')

data = shuffle(df)

for _ in range(100):

    data = shuffle(df)
    L = list(df.columns.values)
    L.pop()

    trainSet, testSet = train_test_split(data, test_size=0.2, random_state=3, shuffle=True)

    decTree = decision_tree(trainSet, L)


    curTrainAccuracy = accuracy(decTree, trainSet)
    trainedAccuracy.append(curTrainAccuracy)

    curTestAccuracy = accuracy(decTree, testSet)
    testedAccuracy.append(curTestAccuracy)

print("trained accuracy average: ",np.mean(trainedAccuracy))
print("tested accuracy average: ",np.mean(testedAccuracy))

print("trained accuracy standard deviation: ",np.std(trainedAccuracy))
print("tested accuracy standard deviation: ",np.std(testedAccuracy))

#PLOTTER FUNCTION--------------------------------------------------------------------------------------------------------------
plt.figure(figsize=(10, 5))
plt.hist(trainedAccuracy, bins=20, alpha=0.5, color='blue')
plt.xlabel('Accuracy')
plt.ylabel('Frequency')
plt.title('Training Accuracy Distribution')
plt.grid(True)
plt.show()


plt.figure(figsize=(10, 5))
plt.hist(testedAccuracy, bins=20, alpha=0.5, color='green')
plt.xlabel('Accuracy')
plt.ylabel('Frequency')
plt.title('Testing Accuracy Distribution')
plt.grid(True)
plt.show()